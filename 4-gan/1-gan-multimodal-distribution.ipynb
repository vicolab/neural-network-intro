{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"This area sets up the Jupyter environment.\n",
    "Please do not modify anything in this cell.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Add project to PYTHONPATH for future use\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# Import miscellaneous modules\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Set CSS styling\n",
    "with open('../admin/custom.css', 'r') as f:\n",
    "    style = \"\"\"<style>\\n{}\\n</style>\"\"\".format(f.read())\n",
    "    display(HTML(style))\n",
    "\n",
    "# Plots will be show inside the notebook\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks 1\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "In this notebook we will use what we have learned about artificial neural networks to explore generative modelling with generative adversarial networks.\n",
    "</div>\n",
    "\n",
    "Generative adversarial networks, or GANs, is a generative modelling methodology by Goodfellow et al. [1] from 2014 that has garnered much interest these past few years. It is based on the idea of transforming samples of latent variables $\\mathbf{z}$ to samples $\\mathbf{x}$ of a probability distribution that we would like to learn. The transformation is done via a differentiable function, which typically is defined as an artifical neural network.\n",
    "\n",
    "When viewed through the lens of game theory, a GAN consists of a *generator* and an adversary called the *discriminator*. The generator network $\\mathbf{G}$ produces samples $\\mathbf{x}$ by transforming latent variables $\\mathbf{z}$ with the help of a neural network. The adversary, the discriminator network $\\mathbf{D}$, attempts to discriminate between the samples $\\mathbf{x}$ generated by $\\mathbf{G}$ and the training data. In other words, the discriminator seeks to detect whether the input data is *fake* or *real*. At the same time, the generator attempts to *fool* the discriminator by generating plausible samples. A GAN has converged when the discriminator can no longer differentiate between real data and samples generated by the generator.\n",
    "\n",
    "Distinguishing between fake and real data sounds like something we have done several times before; indeed, it is a binary classification problem. The original formulation of GANs as a zero-sum game can be seen below:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\underset{\\mathbf{G}}{\\arg\\min}\\max_{\\mathbf{D}}\n",
    "\\frac{1}{N}\\sum_{i=1}^{N} \\ln\\mathbf{D}(\\mathbf{x}_i)+\\ln(1-\\mathbf{D}(\\mathbf{G}(\\mathbf{z}_i)))\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "We can see that the discriminator tries to maximise the log-likelihood of giving the correct prediction, whilst the generator tries to minimise this quantity. In practice, the training is done by alternating optimisation between the generator and the discriminator.\n",
    "\n",
    "GANs are notoriously difficult to train, so for this and the next notebook we are going to have to do some simplifications. First, we are going to train on what we consider to be easy datasets:\n",
    "\n",
    "* **Notebook 1**: A 1-d multimodal distribution\n",
    "* **Notebook 2**: The MNIST database of handwritten digits\n",
    "\n",
    "Secondly, most of the implementation will already be done for you; the focus will be on testing out different kinds of network definitions for the generator and the discriminator. This notebook will start with the 1-d multimodal distribution dataset, while the next one will handle the familiar MNIST dataset.\n",
    "\n",
    "[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio “Generative Adversarial Nets” in: Advances in neural information processing systems 2014, pp. 2672–2680"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multimodal distiribution\n",
    "\n",
    "In this notebook we will use a GAN to generate sample from a 1-d multimodal distribution.\n",
    "\n",
    "We will start loading our uni-dimensional data from a CSV file.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>In the following snippet of code we will:</strong>\n",
    "  <ul>\n",
    "    <li>Load data from a CSV file</li>\n",
    "    <li>Plot the normalised data histogram</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import admin.tools as tools\n",
    "\n",
    "data = pd.read_csv('resources/multinomial.csv', index_col=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack the Pandas DataFrames to NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot normalised histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "tools.plot_normalized_histogram(ax,X_data)\n",
    "ax.set_title('Data Histogram')\n",
    "ax.set_xlabel('Data Value')\n",
    "ax.set_ylabel('Normalized frequency')\n",
    "ax.grid()\n",
    "\n",
    "fig.canvas.draw()\n",
    "time.sleep(0.04)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task I: Implement a Generator Network\n",
    "\n",
    "As previously mentioned, the generator network is built to map a latent space to a specific data distribution.\n",
    "\n",
    "In this task we will make a network that has as input a vector of `zdim` dimensions and is mapped to a pre-defined number of outputs. The number of outputs and its shape is defined by the data distribution we are learning.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "  <strong>Task:</strong> :\n",
    "<ul>\n",
    "  <li>Make a network that accepts inputs where the shape is defined by `zdim` $\\rightarrow$ `shape=(z_dim,)`</li>\n",
    "  <li>The number of outputs of your network need to be defined as `nb_outputs`</li>\n",
    "  <li>Reshape the final layer to be in the shape of `output_shape`</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "* Since the data lies in the range [-1,1] try using the 'tanh' as the final activation function.\n",
    "\n",
    "Keras references: [Reshape()](https://keras.io/layers/core/#reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some useful Keras libraries\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "\n",
    "def generator(z_dim, nb_outputs, output_shape):\n",
    "    \n",
    "    # Define the input_noise as a function of Input()\n",
    "    latent_var = None\n",
    "\n",
    "    # Insert the desired amount of layers for your network\n",
    "    x = None\n",
    "\n",
    "    # Map you latest layer to nb_outputs\n",
    "    x = None\n",
    "\n",
    "    # Reshape you data\n",
    "    x = Reshape(output_shape)(x)\n",
    "\n",
    "    model = Model(inputs=latent_var, outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a generative network using the function you just made.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>In the following code snippet we will:</strong>\n",
    "<ul>\n",
    "  <li>Define the number of dimensions of the latent vector $\\mathbf{z}$</li>\n",
    "  <li>Find out the shape of a sample of data</li>\n",
    "  <li>Compute numbers of dimensions in a sample of data</li>\n",
    "  <li>Create the network using your function</li>\n",
    "  <li>Display a summary of your generator network</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the dimension of the latent vector\n",
    "z_dim = 100\n",
    "\n",
    "# Dimension of our sample\n",
    "sample_dimentions = X_data[0].shape\n",
    "\n",
    "# Calculate the number of dimensions in a sample\n",
    "n_dimensions=1\n",
    "for x in list(sample_dimentions):\n",
    "    n_dimensions *= x\n",
    "\n",
    "print('A sample of data has shape {} composed of {} dimension(s)'.format(sample_dimentions, n_dimensions))\n",
    "\n",
    "# Create the generative network\n",
    "G = generator(z_dim, n_dimensions, sample_dimentions)\n",
    "\n",
    "# We recommend the followin optimizer\n",
    "g_optim = keras.optimizers.adam(lr=0.002, beta_1=0.5, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# Compile network\n",
    "G.compile (loss='binary_crossentropy', optimizer=g_optim)\n",
    "\n",
    "# Network Summary\n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task II: Implement a Discriminative Network\n",
    "\n",
    "The discriminator network is a simple binary classifier where the output indicates the probability of the input data being real or fake.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Task:</strong>\n",
    "<ul>\n",
    "  <li> Create a network where the input shape is `input_shape`\n",
    "  <li> We recomend reshaping your network  just after input. This way you can have a vector with shape `(None, nb_inputs)`</li>\n",
    "  <li> Implement a simple network that can classify data</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Keras references: [Reshape()](https://keras.io/layers/core/#reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(input_shape, nb_inputs):\n",
    "    # Define the network input to have input_shape shape\n",
    "    input_x = None\n",
    "    \n",
    "    # Reshape the input\n",
    "    x = None\n",
    "\n",
    "    # Implement the rest of you classifier\n",
    "    x = None\n",
    "\n",
    "    # Get the output activation (binary classification)\n",
    "    probabilities = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input_x, outputs=probabilities)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a discriminator network using the function you just made.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<strong>In the following code snippet we will:</strong>\n",
    "<ul>\n",
    "  <li>Create the network using your function</li>\n",
    "  <li>Display a summary of your generator network</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We already computed the shape and number of dimensions in a data sample\n",
    "print('The data has shape {} composed of {} dimension(s)'.format(sample_dimentions, n_dimensions))\n",
    "\n",
    "# Discriminative network\n",
    "D = discriminator(sample_dimentions, n_dimensions)\n",
    "\n",
    "# Recommended optimiser\n",
    "d_optim = keras.optimizers.adam(lr=0.002, beta_1=0.5, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# Compile network\n",
    "D.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "\n",
    "# Network summary\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the GAN Together\n",
    "\n",
    "In the following code we will put the generator and discriminator together so we can train our  adversarial model.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<strong>In the following code snippet we will:</strong>\n",
    "<ul>\n",
    "  <li>Use the generator and discriminator to construct a GAN</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "def build(generator, discriminator):\n",
    "    \"\"\"Build a base model for a Generative Adversarial Networks.\n",
    "    Parameters\n",
    "    ----------\n",
    "    generator : keras.engine.training.Model\n",
    "        A keras model built either with keras.models ( Model, or Sequential ).\n",
    "        This is the model that generates the data for the Generative Adversarial networks.\n",
    "    Discriminator : keras.engine.training.Model\n",
    "        A keras model built either with keras.models ( Model, or Sequential ).\n",
    "        This is the model that is a binary classifier for REAL/GENERATED data.\n",
    "    Returns\n",
    "    -------\n",
    "    (keras.engine.training.Model)\n",
    "        It returns a Sequential Keras Model by connecting a Generator model to a\n",
    "        Discriminator model.  [ generator-->discriminator]\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create GAN\n",
    "G_plus_D = build(G, D)\n",
    "G_plus_D.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "D.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task III: Define Hyperparameters\n",
    "\n",
    "Please define the following hyper-parameters to train your GAN.\n",
    "<br>\n",
    "<div class=\"alert alert-success\">\n",
    "  <strong>Task:</strong> Please define the following hyperparameters to train your GAN:\n",
    "  <ul>\n",
    "  <li> Batch size</li>\n",
    "  <li>Number of training epochs</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NB_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>In the following code snippet we will:</strong>\n",
    "<ul>\n",
    "  <li>Train the constructed GAN</li>\n",
    "  <li>Live plot the histogram of the generated data</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Figure for live plot\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# Allocate space for noise variable\n",
    "z = np.zeros((BATCH_SIZE, z_dim))\n",
    "\n",
    "# n_bathces\n",
    "number_of_batches = int(X_data.shape[0] / BATCH_SIZE)\n",
    "\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    for index in range(number_of_batches):\n",
    "        # Sample minimibath m=BATCH_SIZE from data generating distribution\n",
    "        # in other words :\n",
    "        # Grab a batch of the real data\n",
    "        data_batch = X_data[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "\n",
    "        # Sample minibatch of m= BATCH_SIZE noise samples\n",
    "        # in other words, we sample from a uniform distribution\n",
    "        z = np.random.uniform(-1, 1, (BATCH_SIZE,z_dim))\n",
    "\n",
    "        # Sample minibatch m=BATCH_SIZE from data generating distribution Pdata\n",
    "        # in ohter words\n",
    "        # Use genrator to create new fake samples\n",
    "        generated_batch = G.predict(z, verbose=0)\n",
    "\n",
    "        # Update/Train discriminator D\n",
    "        X = np.concatenate((data_batch, generated_batch))\n",
    "        y = [1] * BATCH_SIZE + [0.0] * BATCH_SIZE\n",
    "\n",
    "        d_loss = D.train_on_batch(X, y)\n",
    "\n",
    "        # Sample minibatch of m= BATCH_SIZE noise samples\n",
    "        # in other words, we sample from a uniform distribution\n",
    "        z = np.random.uniform(-1, 1, (BATCH_SIZE,z_dim))\n",
    "        \n",
    "        #Update Generator while not updating discriminator\n",
    "        D.trainable = False\n",
    "        # to do gradient ascent we just flip the labels ...\n",
    "        g_loss = G_plus_D.train_on_batch(z, [1] * BATCH_SIZE)\n",
    "        D.trainable = True\n",
    "        \n",
    "        # Plot data every 10 mini batches\n",
    "        if index % 10 == 0:\n",
    "            ax.clear() \n",
    "            \n",
    "            # Histogram of generated data\n",
    "            tools.plot_normalized_histogram(ax , generated_batch.flatten(), color='b',label='Generated')\n",
    "            \n",
    "            # Histogram of real data\n",
    "            tools.plot_normalized_histogram(ax , X_data, color='y',label='Real')\n",
    "\n",
    "            \n",
    "            # Plot details\n",
    "            ax.legend()\n",
    "            ax.grid()\n",
    "            ax.set_xlim([-1,1])\n",
    "\n",
    "            fig.canvas.draw()\n",
    "            time.sleep(0.01)\n",
    "\n",
    "\n",
    "    # End of epoch ....\n",
    "    print(\"epoch %d : g_loss : %f  | d_loss : %f\" % (epoch, g_loss,  d_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
